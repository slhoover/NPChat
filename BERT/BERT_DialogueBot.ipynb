{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-DialogueBot\n",
        "\n",
        "Colab options: T4 GPU (The CUDA nvidia driver seems to work only with the GPU and not the TPU).\n",
        "\n",
        "Source: https://medium.com/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa"
      ],
      "metadata": {
        "id": "L27KGR7fUpic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Packages"
      ],
      "metadata": {
        "id": "dbr1-65cUyD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7IlNMeKUqYJ",
        "outputId": "84563b26-2839-42c9-ff6f-5bcd9b35abf6"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi4UBOaQ58L6",
        "outputId": "66d4441c-e205-4b8d-abed-714cdc0d4a55"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNQjZriD52Ba",
        "outputId": "0adfe5e6-c11d-44e1-a477-06b4c3cfc08a"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp7-ReuXWP93",
        "outputId": "e371b04e-6cc7-447e-ab8e-ce77319c6d6c"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-dependencies Transformers==3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpWUXGjc5WmZ",
        "outputId": "2d5632f1-5b71-4247-fb7b-66478441e16c"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Transformers==3 in /usr/local/lib/python3.10/dist-packages (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "fSgaRsDY49bi"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "1_QB1jl-6aMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_parquet('sample_data/train-00000-of-00001-4eeea4877d4ce970.parquet')\n",
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "L2u_sR5J5IKz",
        "outputId": "94dffd9a-ef14-44b9-a688-454208c32779"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            Name  \\\n",
              "0                         Bikram   \n",
              "1     Arcturus the Bounty Hunter   \n",
              "2                  Elena Gilbert   \n",
              "3                Arin the Cleric   \n",
              "4                   Scott McCall   \n",
              "...                          ...   \n",
              "1718                          K2   \n",
              "1719           Garrick Stonefist   \n",
              "1720          Luna the Enchanter   \n",
              "1721       Michael \"Mike\" Harper   \n",
              "1722               Elena Gilbert   \n",
              "\n",
              "                                              Biography  \\\n",
              "0     Bikram is a rough and tough smuggler from the ...   \n",
              "1     Arcturus is a fearless bounty hunter who has m...   \n",
              "2     Elena Gilbert is a teenage girl from the town ...   \n",
              "3     Arin is a devoted follower of the god of light...   \n",
              "4     Scott McCall is a teenage werewolf and the mai...   \n",
              "...                                                 ...   \n",
              "1718  K2 is a former special forces soldier turned b...   \n",
              "1719  Garrick is a dwarf paladin who comes from a lo...   \n",
              "1720  Luna is a powerful enchanter who has mastered ...   \n",
              "1721  Michael Harper, also known as \"Mike the Knife\"...   \n",
              "1722  Elena Gilbert is a teenage girl from the town ...   \n",
              "\n",
              "                                                  Query  \\\n",
              "0                   What is your opinion on friendship?   \n",
              "1                 What made you become a bounty hunter?   \n",
              "2     Can you tell me about Elena's role in Mystic F...   \n",
              "3     Have you ever faced a great challenge in your ...   \n",
              "4            Can you tell me about your transformation?   \n",
              "...                                                 ...   \n",
              "1718                        Have you ever felt sadness?   \n",
              "1719               What is Garrick's greatest strength?   \n",
              "1720     What is your opinion on humans who fear magic?   \n",
              "1721    What is Mike's ultimate goal in the apocalypse?   \n",
              "1722  Can you describe Elena's relationship with her...   \n",
              "\n",
              "                                               Response        Emotion  \n",
              "0             Friendship is a bond stronger than blood.        Loyalty  \n",
              "1     I believe in justice and making the world a sa...        Purpose  \n",
              "2                  Elena is often caught in the middle.        Neutral  \n",
              "3        Yes, I've faced trials that tested my beliefs.  Determination  \n",
              "4                   \"Bitten, changed, became werewolf.\"     Acceptance  \n",
              "...                                                 ...            ...  \n",
              "1718               \"Sadness, rare, but understandable.\"        Empathy  \n",
              "1719  Garrick's greatest strength is his unwavering ...           Fear  \n",
              "1720  It's understandable, but magic can also bring ...  Understanding  \n",
              "1721  Mike's goal is to find a safe haven, end the z...           Hope  \n",
              "1722                         They are important to her.      Happiness  \n",
              "\n",
              "[1723 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8ae895e-137d-47de-91d9-0e23c3d45feb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Biography</th>\n",
              "      <th>Query</th>\n",
              "      <th>Response</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your opinion on friendship?</td>\n",
              "      <td>Friendship is a bond stronger than blood.</td>\n",
              "      <td>Loyalty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Arcturus the Bounty Hunter</td>\n",
              "      <td>Arcturus is a fearless bounty hunter who has m...</td>\n",
              "      <td>What made you become a bounty hunter?</td>\n",
              "      <td>I believe in justice and making the world a sa...</td>\n",
              "      <td>Purpose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Elena Gilbert</td>\n",
              "      <td>Elena Gilbert is a teenage girl from the town ...</td>\n",
              "      <td>Can you tell me about Elena's role in Mystic F...</td>\n",
              "      <td>Elena is often caught in the middle.</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Arin the Cleric</td>\n",
              "      <td>Arin is a devoted follower of the god of light...</td>\n",
              "      <td>Have you ever faced a great challenge in your ...</td>\n",
              "      <td>Yes, I've faced trials that tested my beliefs.</td>\n",
              "      <td>Determination</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott McCall</td>\n",
              "      <td>Scott McCall is a teenage werewolf and the mai...</td>\n",
              "      <td>Can you tell me about your transformation?</td>\n",
              "      <td>\"Bitten, changed, became werewolf.\"</td>\n",
              "      <td>Acceptance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1718</th>\n",
              "      <td>K2</td>\n",
              "      <td>K2 is a former special forces soldier turned b...</td>\n",
              "      <td>Have you ever felt sadness?</td>\n",
              "      <td>\"Sadness, rare, but understandable.\"</td>\n",
              "      <td>Empathy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>Garrick Stonefist</td>\n",
              "      <td>Garrick is a dwarf paladin who comes from a lo...</td>\n",
              "      <td>What is Garrick's greatest strength?</td>\n",
              "      <td>Garrick's greatest strength is his unwavering ...</td>\n",
              "      <td>Fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1720</th>\n",
              "      <td>Luna the Enchanter</td>\n",
              "      <td>Luna is a powerful enchanter who has mastered ...</td>\n",
              "      <td>What is your opinion on humans who fear magic?</td>\n",
              "      <td>It's understandable, but magic can also bring ...</td>\n",
              "      <td>Understanding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1721</th>\n",
              "      <td>Michael \"Mike\" Harper</td>\n",
              "      <td>Michael Harper, also known as \"Mike the Knife\"...</td>\n",
              "      <td>What is Mike's ultimate goal in the apocalypse?</td>\n",
              "      <td>Mike's goal is to find a safe haven, end the z...</td>\n",
              "      <td>Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1722</th>\n",
              "      <td>Elena Gilbert</td>\n",
              "      <td>Elena Gilbert is a teenage girl from the town ...</td>\n",
              "      <td>Can you describe Elena's relationship with her...</td>\n",
              "      <td>They are important to her.</td>\n",
              "      <td>Happiness</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1723 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8ae895e-137d-47de-91d9-0e23c3d45feb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b8ae895e-137d-47de-91d9-0e23c3d45feb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b8ae895e-137d-47de-91d9-0e23c3d45feb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0ca9f1cd-65fd-46e8-be7b-fb1f17ed80a8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0ca9f1cd-65fd-46e8-be7b-fb1f17ed80a8')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0ca9f1cd-65fd-46e8-be7b-fb1f17ed80a8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_611882a2-5a2d-4dfa-98f5-1ad435591ebf\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_611882a2-5a2d-4dfa-98f5-1ad435591ebf button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "by_name = df_train.groupby('Name')"
      ],
      "metadata": {
        "id": "XhOw8DzB66D0"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "by_name.get_group('Bikram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "-cBWWJ8F7AEq",
        "outputId": "79bb857a-95c9-4f53-f247-959523f01de1"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Name                                          Biography  \\\n",
              "0     Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "34    Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "58    Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "197   Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "223   Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "664   Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "690   Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "824   Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1066  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1127  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1141  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1175  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1220  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1259  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1417  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1489  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1521  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1603  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1639  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "1706  Bikram  Bikram is a rough and tough smuggler from the ...   \n",
              "\n",
              "                                                  Query  \\\n",
              "0                   What is your opinion on friendship?   \n",
              "34                      What is your opinion on wealth?   \n",
              "58                  What is your ultimate goal in life?   \n",
              "197   What is your favorite place to hide from the law?   \n",
              "223                         Have you ever been in love?   \n",
              "664                    What is your opinion on the law?   \n",
              "690         Have you ever faced an unbeatable opponent?   \n",
              "824                  Have you ever had to kill someone?   \n",
              "1066                  Have you ever pulled a job alone?   \n",
              "1127          Can you tell me about your biggest score?   \n",
              "1141                     What is your biggest weakness?   \n",
              "1175                     What is your preferred weapon?   \n",
              "1220                      Have you ever been in prison?   \n",
              "1259       What do you think of your partners in crime?   \n",
              "1417           Have you ever been caught by the police?   \n",
              "1489                     What is your favorite pastime?   \n",
              "1521                 Have you ever been double-crossed?   \n",
              "1603              What do you do when things get tough?   \n",
              "1639                  Have you ever faced a rival gang?   \n",
              "1706            What is your opinion on the government?   \n",
              "\n",
              "                                               Response        Emotion  \n",
              "0             Friendship is a bond stronger than blood.        Loyalty  \n",
              "34                         Wealth is power and freedom.       Ambition  \n",
              "58    My ultimate goal is to live life on my own terms.      Rebellion  \n",
              "197         My favorite place is the city's underworld.    Familiarity  \n",
              "223                   Love is a luxury I cannot afford.   Practicality  \n",
              "664               The law is a hindrance to my freedom.      Rebellion  \n",
              "690   I have faced many opponents, but I am always t...     Confidence  \n",
              "824     I have had to eliminate threats to my survival.   Practicality  \n",
              "1066                     I have pulled many jobs alone.   Independence  \n",
              "1127           My biggest score was a shipment of gold.          Pride  \n",
              "1141   My biggest weakness is my loyalty to my friends.  Vulnerability  \n",
              "1175                 My preferred weapon is a revolver.    Familiarity  \n",
              "1220        I have been in prison, but I always escape.  Determination  \n",
              "1259                         My partners are my family.        Loyalty  \n",
              "1417   I have been caught, but I always find a way out.     Confidence  \n",
              "1489     My favorite pastime is planning my next score.   Anticipation  \n",
              "1521  I have been double-crossed, but I always get m...          Anger  \n",
              "1603                I use my wit to get out of trouble.     Confidence  \n",
              "1639  I have faced many rival gangs and emerged vict...     Confidence  \n",
              "1706          The government is corrupt and oppressive.        Disgust  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27c25b6b-2854-480d-93cc-729d3f45e94d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Biography</th>\n",
              "      <th>Query</th>\n",
              "      <th>Response</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your opinion on friendship?</td>\n",
              "      <td>Friendship is a bond stronger than blood.</td>\n",
              "      <td>Loyalty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your opinion on wealth?</td>\n",
              "      <td>Wealth is power and freedom.</td>\n",
              "      <td>Ambition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your ultimate goal in life?</td>\n",
              "      <td>My ultimate goal is to live life on my own terms.</td>\n",
              "      <td>Rebellion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your favorite place to hide from the law?</td>\n",
              "      <td>My favorite place is the city's underworld.</td>\n",
              "      <td>Familiarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever been in love?</td>\n",
              "      <td>Love is a luxury I cannot afford.</td>\n",
              "      <td>Practicality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your opinion on the law?</td>\n",
              "      <td>The law is a hindrance to my freedom.</td>\n",
              "      <td>Rebellion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>690</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever faced an unbeatable opponent?</td>\n",
              "      <td>I have faced many opponents, but I am always t...</td>\n",
              "      <td>Confidence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever had to kill someone?</td>\n",
              "      <td>I have had to eliminate threats to my survival.</td>\n",
              "      <td>Practicality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever pulled a job alone?</td>\n",
              "      <td>I have pulled many jobs alone.</td>\n",
              "      <td>Independence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Can you tell me about your biggest score?</td>\n",
              "      <td>My biggest score was a shipment of gold.</td>\n",
              "      <td>Pride</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your biggest weakness?</td>\n",
              "      <td>My biggest weakness is my loyalty to my friends.</td>\n",
              "      <td>Vulnerability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1175</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your preferred weapon?</td>\n",
              "      <td>My preferred weapon is a revolver.</td>\n",
              "      <td>Familiarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever been in prison?</td>\n",
              "      <td>I have been in prison, but I always escape.</td>\n",
              "      <td>Determination</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1259</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What do you think of your partners in crime?</td>\n",
              "      <td>My partners are my family.</td>\n",
              "      <td>Loyalty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1417</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever been caught by the police?</td>\n",
              "      <td>I have been caught, but I always find a way out.</td>\n",
              "      <td>Confidence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your favorite pastime?</td>\n",
              "      <td>My favorite pastime is planning my next score.</td>\n",
              "      <td>Anticipation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever been double-crossed?</td>\n",
              "      <td>I have been double-crossed, but I always get m...</td>\n",
              "      <td>Anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1603</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What do you do when things get tough?</td>\n",
              "      <td>I use my wit to get out of trouble.</td>\n",
              "      <td>Confidence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1639</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>Have you ever faced a rival gang?</td>\n",
              "      <td>I have faced many rival gangs and emerged vict...</td>\n",
              "      <td>Confidence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1706</th>\n",
              "      <td>Bikram</td>\n",
              "      <td>Bikram is a rough and tough smuggler from the ...</td>\n",
              "      <td>What is your opinion on the government?</td>\n",
              "      <td>The government is corrupt and oppressive.</td>\n",
              "      <td>Disgust</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27c25b6b-2854-480d-93cc-729d3f45e94d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-27c25b6b-2854-480d-93cc-729d3f45e94d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-27c25b6b-2854-480d-93cc-729d3f45e94d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6087ba18-ad79-4f49-859b-c79ce77cd9af\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6087ba18-ad79-4f49-859b-c79ce77cd9af')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6087ba18-ad79-4f49-859b-c79ce77cd9af button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "by_name.get_group('Bikram')['Emotion'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijv54Yv37Ow1",
        "outputId": "3353ef8f-8e8e-4bcd-f813-dc4c77621e25"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Confidence       4\n",
              "Loyalty          2\n",
              "Rebellion        2\n",
              "Familiarity      2\n",
              "Practicality     2\n",
              "Ambition         1\n",
              "Independence     1\n",
              "Pride            1\n",
              "Vulnerability    1\n",
              "Determination    1\n",
              "Anticipation     1\n",
              "Anger            1\n",
              "Disgust          1\n",
              "Name: Emotion, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "by_name.get_group('Bikram')['Emotion'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hoPGs-X7XS7",
        "outputId": "341c33ad-87f9-4156-af51-ef735fe89dba"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = by_name.get_group('Bikram').to_dict()"
      ],
      "metadata": {
        "id": "sI0uYsqy7vgl"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Query']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX07a4iV9ld4",
        "outputId": "cff16b3f-af9c-45a5-b491-6beda7dd08ec"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'What is your opinion on friendship?',\n",
              " 34: 'What is your opinion on wealth?',\n",
              " 58: 'What is your ultimate goal in life?',\n",
              " 197: 'What is your favorite place to hide from the law?',\n",
              " 223: 'Have you ever been in love?',\n",
              " 664: 'What is your opinion on the law?',\n",
              " 690: 'Have you ever faced an unbeatable opponent?',\n",
              " 824: 'Have you ever had to kill someone?',\n",
              " 1066: 'Have you ever pulled a job alone?',\n",
              " 1127: 'Can you tell me about your biggest score?',\n",
              " 1141: 'What is your biggest weakness?',\n",
              " 1175: 'What is your preferred weapon?',\n",
              " 1220: 'Have you ever been in prison?',\n",
              " 1259: 'What do you think of your partners in crime?',\n",
              " 1417: 'Have you ever been caught by the police?',\n",
              " 1489: 'What is your favorite pastime?',\n",
              " 1521: 'Have you ever been double-crossed?',\n",
              " 1603: 'What do you do when things get tough?',\n",
              " 1639: 'Have you ever faced a rival gang?',\n",
              " 1706: 'What is your opinion on the government?'}"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdZSfo-692Yw",
        "outputId": "93ea4769-5d88-4717-cf83-9d1be1c255be"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Friendship is a bond stronger than blood.',\n",
              " 34: 'Wealth is power and freedom.',\n",
              " 58: 'My ultimate goal is to live life on my own terms.',\n",
              " 197: \"My favorite place is the city's underworld.\",\n",
              " 223: 'Love is a luxury I cannot afford.',\n",
              " 664: 'The law is a hindrance to my freedom.',\n",
              " 690: 'I have faced many opponents, but I am always the victor.',\n",
              " 824: 'I have had to eliminate threats to my survival.',\n",
              " 1066: 'I have pulled many jobs alone.',\n",
              " 1127: 'My biggest score was a shipment of gold.',\n",
              " 1141: 'My biggest weakness is my loyalty to my friends.',\n",
              " 1175: 'My preferred weapon is a revolver.',\n",
              " 1220: 'I have been in prison, but I always escape.',\n",
              " 1259: 'My partners are my family.',\n",
              " 1417: 'I have been caught, but I always find a way out.',\n",
              " 1489: 'My favorite pastime is planning my next score.',\n",
              " 1521: 'I have been double-crossed, but I always get my revenge.',\n",
              " 1603: 'I use my wit to get out of trouble.',\n",
              " 1639: 'I have faced many rival gangs and emerged victorious.',\n",
              " 1706: 'The government is corrupt and oppressive.'}"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Emotion']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sueQNzy-Ic9",
        "outputId": "97621ac1-ace2-4fdd-f8fb-d33d28b75683"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Loyalty',\n",
              " 34: 'Ambition',\n",
              " 58: 'Rebellion',\n",
              " 197: 'Familiarity',\n",
              " 223: 'Practicality',\n",
              " 664: 'Rebellion',\n",
              " 690: 'Confidence',\n",
              " 824: 'Practicality',\n",
              " 1066: 'Independence',\n",
              " 1127: 'Pride',\n",
              " 1141: 'Vulnerability',\n",
              " 1175: 'Familiarity',\n",
              " 1220: 'Determination',\n",
              " 1259: 'Loyalty',\n",
              " 1417: 'Confidence',\n",
              " 1489: 'Anticipation',\n",
              " 1521: 'Anger',\n",
              " 1603: 'Confidence',\n",
              " 1639: 'Confidence',\n",
              " 1706: 'Disgust'}"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion = {}\n",
        "for id, _emotion in data['Emotion'].items():\n",
        "  if not _emotion in emotion:\n",
        "    emotion[_emotion] = []\n",
        "  emotion[_emotion].append(data['Response'][id])"
      ],
      "metadata": {
        "id": "5x-_LJ-t-Pm2"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwb24od__n4c",
        "outputId": "fec72711-b810-40a1-9f83-fe965f952227"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Loyalty': ['Friendship is a bond stronger than blood.',\n",
              "  'My partners are my family.'],\n",
              " 'Ambition': ['Wealth is power and freedom.'],\n",
              " 'Rebellion': ['My ultimate goal is to live life on my own terms.',\n",
              "  'The law is a hindrance to my freedom.'],\n",
              " 'Familiarity': [\"My favorite place is the city's underworld.\",\n",
              "  'My preferred weapon is a revolver.'],\n",
              " 'Practicality': ['Love is a luxury I cannot afford.',\n",
              "  'I have had to eliminate threats to my survival.'],\n",
              " 'Confidence': ['I have faced many opponents, but I am always the victor.',\n",
              "  'I have been caught, but I always find a way out.',\n",
              "  'I use my wit to get out of trouble.',\n",
              "  'I have faced many rival gangs and emerged victorious.'],\n",
              " 'Independence': ['I have pulled many jobs alone.'],\n",
              " 'Pride': ['My biggest score was a shipment of gold.'],\n",
              " 'Vulnerability': ['My biggest weakness is my loyalty to my friends.'],\n",
              " 'Determination': ['I have been in prison, but I always escape.'],\n",
              " 'Anticipation': ['My favorite pastime is planning my next score.'],\n",
              " 'Anger': ['I have been double-crossed, but I always get my revenge.'],\n",
              " 'Disgust': ['The government is corrupt and oppressive.']}"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "data[\"intents\"] = []\n",
        "\n",
        "for _emotion in emotion.keys():\n",
        "  _data = {}\n",
        "  _data[\"tag\"] = _emotion\n",
        "  _data[\"responses\"] = emotion[_emotion]\n",
        "  data[\"intents\"].append(_data)"
      ],
      "metadata": {
        "id": "HwBVIhws_xe1"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H5OWyIuAcoe",
        "outputId": "6d387c23-6eda-426d-a1bc-af549372c853"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'Loyalty',\n",
              "   'responses': ['Friendship is a bond stronger than blood.',\n",
              "    'My partners are my family.']},\n",
              "  {'tag': 'Ambition', 'responses': ['Wealth is power and freedom.']},\n",
              "  {'tag': 'Rebellion',\n",
              "   'responses': ['My ultimate goal is to live life on my own terms.',\n",
              "    'The law is a hindrance to my freedom.']},\n",
              "  {'tag': 'Familiarity',\n",
              "   'responses': [\"My favorite place is the city's underworld.\",\n",
              "    'My preferred weapon is a revolver.']},\n",
              "  {'tag': 'Practicality',\n",
              "   'responses': ['Love is a luxury I cannot afford.',\n",
              "    'I have had to eliminate threats to my survival.']},\n",
              "  {'tag': 'Confidence',\n",
              "   'responses': ['I have faced many opponents, but I am always the victor.',\n",
              "    'I have been caught, but I always find a way out.',\n",
              "    'I use my wit to get out of trouble.',\n",
              "    'I have faced many rival gangs and emerged victorious.']},\n",
              "  {'tag': 'Independence', 'responses': ['I have pulled many jobs alone.']},\n",
              "  {'tag': 'Pride', 'responses': ['My biggest score was a shipment of gold.']},\n",
              "  {'tag': 'Vulnerability',\n",
              "   'responses': ['My biggest weakness is my loyalty to my friends.']},\n",
              "  {'tag': 'Determination',\n",
              "   'responses': ['I have been in prison, but I always escape.']},\n",
              "  {'tag': 'Anticipation',\n",
              "   'responses': ['My favorite pastime is planning my next score.']},\n",
              "  {'tag': 'Anger',\n",
              "   'responses': ['I have been double-crossed, but I always get my revenge.']},\n",
              "  {'tag': 'Disgust',\n",
              "   'responses': ['The government is corrupt and oppressive.']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = by_name.get_group('Bikram')"
      ],
      "metadata": {
        "id": "AZGIdbgcA53n"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the labels into encodings\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['Emotion'])\n",
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTmCj0yAAhrG",
        "outputId": "01276b69-c0cb-4c85-8809-7a474c8fad03"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-210-c6f7a85cdd17>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['label'] = le.fit_transform(df['Emotion'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3     0.20\n",
              "8     0.10\n",
              "11    0.10\n",
              "6     0.10\n",
              "9     0.10\n",
              "0     0.05\n",
              "7     0.05\n",
              "10    0.05\n",
              "12    0.05\n",
              "4     0.05\n",
              "2     0.05\n",
              "1     0.05\n",
              "5     0.05\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, train_labels = df['Query'], df['label']"
      ],
      "metadata": {
        "id": "vXR5Grt4BD5F"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling"
      ],
      "metadata": {
        "id": "TaMF_6_HWBfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "# Load the DistilBert tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# Import the DistilBert pretrained model\n",
        "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "PlQ4qyEpBUHo"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "pd.Series(seq_len).hist(bins = 10)\n",
        "# Based on the histogram we are selecting the max len as 8\n",
        "max_seq_len = 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "rQC3BvFSBef3",
        "outputId": "1e257316-ed49-405c-d4db-0916f9819d9d"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "id": "OEoarrfOBwJx"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())"
      ],
      "metadata": {
        "id": "65bBLie_B02V"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#define a batch size\n",
        "batch_size = 16\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "#train_sampler = train_data\n",
        "# DataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-6uCYPs2CMeu"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "   def __init__(self, bert):\n",
        "       super(BERT_Arch, self).__init__()\n",
        "       self.bert = bert\n",
        "\n",
        "       # dropout layer\n",
        "       self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "       # relu activation function\n",
        "       self.relu =  nn.ReLU()\n",
        "       # dense layer\n",
        "       self.fc1 = nn.Linear(768,512)\n",
        "       self.fc2 = nn.Linear(512,256)\n",
        "       self.fc3 = nn.Linear(256,13)\n",
        "       #self.fc3 = nn.Linear(256,5)\n",
        "       #softmax activation function\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "       #define the forward pass\n",
        "   def forward(self, sent_id, mask):\n",
        "      #pass the inputs to the model\n",
        "      cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]\n",
        "\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      # output layer\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "vIHPisqICQEO"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters. This will prevent updating of model weights during fine-tuning.\n",
        "for param in bert.parameters():\n",
        "      param.requires_grad = False\n",
        "model = BERT_Arch(bert)\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1-ctavPCTll",
        "outputId": "c77ccee8-c520-4ac2-d46b-f87f94dc3be6"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================\n",
              "Layer (type:depth-idx)                                  Param #\n",
              "================================================================================\n",
              "BERT_Arch                                               --\n",
              "├─DistilBertModel: 1-1                                  --\n",
              "│    └─Embeddings: 2-1                                  --\n",
              "│    │    └─Embedding: 3-1                              (23,440,896)\n",
              "│    │    └─Embedding: 3-2                              (393,216)\n",
              "│    │    └─LayerNorm: 3-3                              (1,536)\n",
              "│    │    └─Dropout: 3-4                                --\n",
              "│    └─Transformer: 2-2                                 --\n",
              "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
              "├─Dropout: 1-2                                          --\n",
              "├─ReLU: 1-3                                             --\n",
              "├─Linear: 1-4                                           393,728\n",
              "├─Linear: 1-5                                           131,328\n",
              "├─Linear: 1-6                                           3,341\n",
              "├─LogSoftmax: 1-7                                       --\n",
              "================================================================================\n",
              "Total params: 66,891,277\n",
              "Trainable params: 528,397\n",
              "Non-trainable params: 66,362,880\n",
              "================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "sa2kcSBECiRh"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "#compute the class weights\n",
        "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "print(class_wts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snadGELpCmLQ",
        "outputId": "6fa4a194-3ace-4912-9c1f-d4be0a920641"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.53846154 1.53846154 1.53846154 0.38461538 1.53846154 1.53846154\n",
            " 0.76923077 1.53846154 0.76923077 0.76923077 1.53846154 0.76923077\n",
            " 1.53846154]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "# loss function\n",
        "cross_entropy = nn.NLLLoss(weight=weights)"
      ],
      "metadata": {
        "id": "YqVmw8yBC_os"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "# number of training epochs\n",
        "epochs = 200\n",
        "# We can also use learning rate scheduler to achieve better results\n",
        "lr_sch = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
      ],
      "metadata": {
        "id": "kVo14vefDFJ9"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  print(\"Done\")\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step,    len(train_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # get model predictions for the current batch\n",
        "\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    #print(\"train DONE\")\n",
        "    #print(len(labels), len(preds))\n",
        "\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    print(\"Loss DONE\")\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # clear calculated gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # We are not using learning rate scheduler as of now\n",
        "    # lr_sch.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  #returns the loss and predictions\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "ve57fp8xDZAP"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss,_ = train()\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "print(f'\\nTraining Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phTJpO_cD3ff",
        "outputId": "d6b8b405-f5d3-4d6d-deb0-1b26fc1e13de"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 2 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 3 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 4 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 5 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 6 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 7 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 8 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 9 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 10 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 11 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 12 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 13 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 14 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 15 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 16 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 17 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 18 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 19 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 20 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 21 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 22 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 23 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 24 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 25 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 26 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 27 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 28 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 29 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 30 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 31 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 32 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 33 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 34 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 35 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 36 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 37 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 38 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 39 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 40 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 41 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 42 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 43 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 44 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 45 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 46 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 47 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 48 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 49 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 50 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 51 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 52 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 53 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 54 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 55 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 56 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 57 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 58 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 59 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 60 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 61 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 62 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 63 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 64 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 65 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 66 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 67 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 68 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 69 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 70 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 71 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 72 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 73 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 74 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 75 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 76 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 77 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 78 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 79 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 80 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 81 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 82 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 83 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 84 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 85 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 86 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 87 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 88 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 89 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 90 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 91 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 92 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 93 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 94 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 95 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 96 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 97 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 98 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 99 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 100 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 101 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 102 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 103 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 104 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 105 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 106 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 107 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 108 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 109 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 110 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 111 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 112 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 113 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 114 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 115 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 116 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 117 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 118 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 119 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 120 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 121 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 122 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 123 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 124 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 125 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 126 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 127 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 128 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 129 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 130 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 131 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 132 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 133 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 134 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 135 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 136 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 137 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 138 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 139 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 140 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 141 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 142 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 143 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 144 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 145 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 146 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 147 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 148 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 149 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 150 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 151 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 152 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 153 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 154 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 155 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 156 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 157 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 158 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 159 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 160 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 161 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 162 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 163 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 164 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 165 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 166 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 167 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 168 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 169 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 170 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 171 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 172 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 173 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 174 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 175 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 176 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 177 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 178 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 179 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 180 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 181 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 182 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 183 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 184 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 185 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 186 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 187 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 188 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 189 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 190 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 191 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 192 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 193 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 194 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 195 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 196 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 197 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 198 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 199 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            " Epoch 200 / 200\n",
            "Done\n",
            "Loss DONE\n",
            "Loss DONE\n",
            "\n",
            "Training Loss: 0.036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(str):\n",
        " str = re.sub(r'[^a-zA-Z ]+', '', str)\n",
        " test_text = [str]\n",
        " model.eval()\n",
        "\n",
        " tokens_test_data = tokenizer(\n",
        " test_text,\n",
        " max_length = max_seq_len,\n",
        " pad_to_max_length=True,\n",
        " truncation=True,\n",
        " return_token_type_ids=False\n",
        " )\n",
        " test_seq = torch.tensor(tokens_test_data['input_ids'])\n",
        " test_mask = torch.tensor(tokens_test_data['attention_mask'])\n",
        "\n",
        " preds = None\n",
        " with torch.no_grad():\n",
        "   preds = model(test_seq.to(device), test_mask.to(device))\n",
        " preds = preds.detach().cpu().numpy()\n",
        " preds = np.argmax(preds, axis = 1)\n",
        " print('Intent Identified: ', le.inverse_transform(preds)[0])\n",
        " return le.inverse_transform(preds)[0]\n",
        "\n",
        "def get_response(message):\n",
        "  intent = get_prediction(message)\n",
        "  for i in data['intents']:\n",
        "    if i[\"tag\"] == intent:\n",
        "      result = random.choice(i[\"responses\"])\n",
        "      break\n",
        "  print(f\"Response : {result}\")\n",
        "  return \"Intent: \"+ intent + '\\n' + \"Response: \" + result"
      ],
      "metadata": {
        "id": "qubfGjSwSbjC"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_response(\"what is your hobby\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "U0rK9JSTStgy",
        "outputId": "bf38dce5-f1ab-45c3-887a-5c0fe9303541"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent Identified:  Anticipation\n",
            "Response : My favorite pastime is planning my next score.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Intent: Anticipation\\nResponse: My favorite pastime is planning my next score.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_response(\"What is your weakness?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vSzxEy1TTI9w",
        "outputId": "9e0d3563-eedf-40e0-b79a-bfa6950e7133"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent Identified:  Confidence\n",
            "Response : I have faced many rival gangs and emerged victorious.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Intent: Confidence\\nResponse: I have faced many rival gangs and emerged victorious.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_response(\"Did police ever catch you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SApgZBt3UBdZ",
        "outputId": "9fd3b329-d97c-46df-8637-cf2d8a232ff9"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent Identified:  Determination\n",
            "Response : I have been in prison, but I always escape.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Intent: Determination\\nResponse: I have been in prison, but I always escape.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    }
  ]
}